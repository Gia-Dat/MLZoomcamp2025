{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0aKmdwsqFLm",
    "outputId": "56056851-dc2d-4429-a4f0-dcb12bca21d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'clothing-dataset-small'...\n",
      "remote: Enumerating objects: 3839, done.\u001b[K\n",
      "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
      "remote: Compressing objects: 100% (400/400), done.\u001b[K\n",
      "remote: Total 3839 (delta 9), reused 385 (delta 0), pack-reused 3439 (from 1)\u001b[K\n",
      "Receiving objects: 100% (3839/3839), 100.58 MiB | 35.39 MiB/s, done.\n",
      "Resolving deltas: 100% (10/10), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/alexeygrigorev/clothing-dataset-small.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N7ZlKtEirA41"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2vqb1TiZrDEb"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BRRJUZgrDL3",
    "outputId": "09f28608-0792-4d47-bfe2-c18bfb2c7170"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('clothing-dataset-small/train/pants/0098b991-e36e-4ef1-b5ee-4154b21e2a92.jpg')\n",
    "img = img.resize((224, 224))\n",
    "x = np.array(img)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IFujhjRArDO8"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoWGV---rDRn",
    "outputId": "8b6749b4-2625-45fc-957b-b25d43c66751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13.6M/13.6M [00:00<00:00, 203MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5MVjTx40rDUQ"
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iHnVkFaorDWw"
   },
   "outputs": [],
   "source": [
    "x = preprocess(img)\n",
    "\n",
    "batch_t = torch.unsqueeze(x, 0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(batch_t)\n",
    "\n",
    "_, indices = torch.sort(output, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyKNBqnvrDZI",
    "outputId": "af625e23-0994-4edc-9428-1a1a2de8a733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-29 01:33:04--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10472 (10K) [text/plain]\n",
      "Saving to: ‘imagenet_classes.txt’\n",
      "\n",
      "\r",
      "imagenet_classes.tx   0%[                    ]       0  --.-KB/s               \r",
      "imagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-11-29 01:33:04 (131 MB/s) - ‘imagenet_classes.txt’ saved [10472/10472]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt -O imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRvfjkIXvtVY",
    "outputId": "b8f64574-134f-4f0f-bb96-ce5913eaad5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "1: jean\n",
      "2: suit\n",
      "3: sweatshirt\n",
      "4: cardigan\n",
      "5: overskirt\n"
     ]
    }
   ],
   "source": [
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# Get top 5 predictions\n",
    "top5_indices = indices[0, :5].tolist()\n",
    "top5_classes = [categories[i] for i in top5_indices]\n",
    "\n",
    "print(\"Top 5 predictions:\")\n",
    "for i, class_name in enumerate(top5_classes):\n",
    "    print(f\"{i+1}: {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UbG-K9xavtYI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for label_name in self.classes:\n",
    "            label_dir = os.path.join(data_dir, label_name)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Iaj-Wb44vtbR"
   },
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "\n",
    "# ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(10),           # Rotate up to 10 degrees\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),  # Zoom\n",
    "    transforms.RandomHorizontalFlip(),       # Horizontal flip\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kvCco_bYyOCt"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ClothingDataset(\n",
    "    data_dir='./clothing-dataset-small/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = ClothingDataset(\n",
    "    data_dir='./clothing-dataset-small/validation',\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gDj6iHbRyOQz"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ClothingClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ClothingClassifierMobileNet, self).__init__()\n",
    "\n",
    "        # Load pre-trained MobileNetV2\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # Freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # Add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.output_layer = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JgRhlfxavtdw"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ClothingClassifierMobileNet(num_classes=10)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mIKL6E0lvtgs"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNzPS5iOPjAS",
    "outputId": "d26de730-b2e5-4360-eb07-f78926202260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  Train Loss: 1.5811, Train Acc: 0.5913\n",
      "  Val Loss: 0.9244, Val Acc: 0.7390\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.9162, Train Acc: 0.7314\n",
      "  Val Loss: 0.8502, Val Acc: 0.7214\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.7992, Train Acc: 0.7595\n",
      "  Val Loss: 0.9293, Val Acc: 0.7449\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.7405, Train Acc: 0.7793\n",
      "  Val Loss: 1.2607, Val Acc: 0.7097\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.7259, Train Acc: 0.7947\n",
      "  Val Loss: 1.0389, Val Acc: 0.7625\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.8682, Train Acc: 0.7627\n",
      "  Val Loss: 1.1845, Val Acc: 0.7331\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.6912, Train Acc: 0.8015\n",
      "  Val Loss: 1.4351, Val Acc: 0.7214\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.6569, Train Acc: 0.8178\n",
      "  Val Loss: 1.2398, Val Acc: 0.7537\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.5469, Train Acc: 0.8276\n",
      "  Val Loss: 1.2291, Val Acc: 0.7507\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.6302, Train Acc: 0.8227\n",
      "  Val Loss: 1.4736, Val Acc: 0.7097\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move data to the specified device (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients to prevent accumulation\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training loss\n",
    "        running_loss += loss.item()\n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Update total and correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    # Disable gradient calculation for validation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the validation data\n",
    "        for inputs, labels in val_loader:\n",
    "            # Move data to the specified device (GPU or CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += loss.item()\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # Update total and correct predictions\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "    print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "K0Lh3YhPvtjk"
   },
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.01):\n",
    "    model = ClothingClassifierMobileNet(num_classes=10)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1boUF4UbT3qI"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device):\n",
    "    best_val_accuracy = 0.0  # Initialize variable to track the best validation accuracy\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            checkpoint_path = f'clothing_v4_{epoch+1:02d}_{val_acc:.3f}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f'Checkpoint saved: {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AcSBNU_nvtmk"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ClothingClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, size_inner=100, droprate=0.2, num_classes=10):\n",
    "        super(ClothingClassifierMobileNet, self).__init__()\n",
    "\n",
    "        # Load pre-trained MobileNetV2\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # Freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # Add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.inner = nn.Linear(1280, size_inner)  # New inner layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(droprate)  # Add dropout\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_model(\n",
    "        learning_rate=0.01,\n",
    "        size_inner=100,\n",
    "        droprate=0.2,\n",
    "):\n",
    "    model = ClothingClassifierMobileNet(\n",
    "        num_classes=10,\n",
    "        size_inner=size_inner,\n",
    "        droprate=droprate,\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sflqY6RIvtpk",
    "outputId": "7e8bc0f1-6171-4407-f30e-2b288b5385a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  Train Loss: 1.5317, Train Acc: 0.4987\n",
      "  Val Loss: 0.9814, Val Acc: 0.6628\n",
      "Checkpoint saved: clothing_v4_01_0.663.pth\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.9773, Train Acc: 0.6737\n",
      "  Val Loss: 0.7821, Val Acc: 0.7273\n",
      "Checkpoint saved: clothing_v4_02_0.727.pth\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.8179, Train Acc: 0.7308\n",
      "  Val Loss: 0.6492, Val Acc: 0.7977\n",
      "Checkpoint saved: clothing_v4_03_0.798.pth\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.7169, Train Acc: 0.7539\n",
      "  Val Loss: 0.6541, Val Acc: 0.7947\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.6624, Train Acc: 0.7722\n",
      "  Val Loss: 0.6132, Val Acc: 0.7947\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.6253, Train Acc: 0.7836\n",
      "  Val Loss: 0.5728, Val Acc: 0.8065\n",
      "Checkpoint saved: clothing_v4_06_0.806.pth\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.6032, Train Acc: 0.7907\n",
      "  Val Loss: 0.6052, Val Acc: 0.7889\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.5660, Train Acc: 0.8018\n",
      "  Val Loss: 0.5707, Val Acc: 0.7859\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.5537, Train Acc: 0.8077\n",
      "  Val Loss: 0.5932, Val Acc: 0.8006\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.5100, Train Acc: 0.8269\n",
      "  Val Loss: 0.5867, Val Acc: 0.7977\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.5004, Train Acc: 0.8230\n",
      "  Val Loss: 0.5776, Val Acc: 0.8065\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.4902, Train Acc: 0.8308\n",
      "  Val Loss: 0.6208, Val Acc: 0.7918\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.4732, Train Acc: 0.8400\n",
      "  Val Loss: 0.6087, Val Acc: 0.8006\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.4847, Train Acc: 0.8272\n",
      "  Val Loss: 0.5824, Val Acc: 0.7947\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.4445, Train Acc: 0.8455\n",
      "  Val Loss: 0.5990, Val Acc: 0.7830\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3918876033.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-3741788033.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2989167225.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"load_seek\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0mseek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_seek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0muse_mmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "model, optimizer = make_model(\n",
    "    learning_rate=0.001,\n",
    "    size_inner=100,\n",
    "    droprate=0.2,\n",
    ")\n",
    "\n",
    "train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TYSlHgskvtsQ"
   },
   "outputs": [],
   "source": [
    "path = '/content/clothing_v4_06_0.806.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bdeiBebxVssM"
   },
   "outputs": [],
   "source": [
    "model = ClothingClassifierMobileNet(size_inner=100, droprate=0.2, num_classes=10)\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "kS-SJKeGVsvX"
   },
   "outputs": [],
   "source": [
    "x = val_transforms(img)\n",
    "batch_t = torch.unsqueeze(x, 0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(batch_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "iuKstC8_Vs37"
   },
   "outputs": [],
   "source": [
    "classes = ['dress',\n",
    " 'hat',\n",
    " 'longsleeve',\n",
    " 'outwear',\n",
    " 'pants',\n",
    " 'shirt',\n",
    " 'shoes',\n",
    " 'shorts',\n",
    " 'skirt',\n",
    " 't-shirt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Dv6nsloVszf",
    "outputId": "1f3f598c-0308-44b9-cb4b-d03eae1013b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': tensor(-0.2632),\n",
       " 'hat': tensor(-3.8934),\n",
       " 'longsleeve': tensor(-0.8155),\n",
       " 'outwear': tensor(-0.3451),\n",
       " 'pants': tensor(5.1578),\n",
       " 'shirt': tensor(-1.6697),\n",
       " 'shoes': tensor(-1.5859),\n",
       " 'shorts': tensor(-0.5320),\n",
       " 'skirt': tensor(-1.6507),\n",
       " 't-shirt': tensor(-2.8391)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(classes, output[0].to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALAAYNBQXA7E",
    "outputId": "edd0198e-18b0-4f39-f2dd-cb1c0c1e4595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
      "Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: onnx\n",
      "Successfully installed onnx-1.19.1\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
